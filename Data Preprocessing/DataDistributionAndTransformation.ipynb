{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMni4UIwLoSqKWlMCxiFiyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuradha-datascience/machine-learning/blob/main/Data%20Preprocessing/DataDistributionAndTransformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What is Normal/Gaussian Distribution\n",
        "- What is Scale of Data\n",
        "- Most ML algo except the decision trees have basic assumption of Normal Data Distribution or atleast numeric variables are on same scale\n",
        "-So what your data distribution looks like for numeric variables\n",
        " - describe, histogram, boxplot, pairplot\n",
        "\n",
        "- data transformation  \n",
        "\n",
        "\n",
        "\n",
        "Scaling Data - linear transformation\n",
        "\n",
        "  - Standarization and Min Max Scaling/normalization\n",
        "    - in min max scaling or normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest.\n",
        "   - When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.)\n",
        "\n",
        "   - While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is.use scikit-learn Standard Scaler\n",
        "\n",
        "   -\n",
        "\n",
        "From these techniques we can scale data linearly, which will not affect the data's shape. This works great if your data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population.\n",
        "\n",
        "- data transformation - Scaling Data - non linear transformation- power transformations\n",
        "-log , square root, box cox, reciprocal\n",
        "-handle skewed data distributions, reduce the range of values, and stabilize variance\n",
        "- scikit-learn power transformer for log transformation\n",
        "- change in the shape of the distribution\n",
        "\n",
        "- Removing Outliers\n",
        "\n",
        "Even after data transformation , distribution may remain skewed - we need to handle outliers(data entry issues or genuine occurences)\n",
        "\n",
        " - Quantile Based Methods- capping at certain quantile -even if no outlier is there , you are removing the top data as defined by percentile.use quantile method of dataframe\n",
        "\n",
        " - standard deviation based detection - like the points away from 3 standard deviation.this ensures only genuinely outlier data is removed.find meand and std.more statistically sound approach\n",
        "\n",
        " - only in very rare cases we would like to remove outliers\n",
        "\n",
        "More on Data Transformation\n",
        "- about fit and transform method\n",
        "\n",
        "- data leakage\n",
        "Using data which you dont have acess while building model and assesing its performance -like test or unseen data\n",
        "When you have deployed your model in production you dont have access to future data\n",
        " - when should we do transformation in ml processing - before or after train test split.thus to avoid data leakage perform these steps only on training data or you will over estimate the accuracy of your model\n",
        "\n",
        "  - scalin -> fit and transform on training data\n",
        "  - for outliers -thresh holds -std and mean from train to be applied on test\n",
        "  - transform unseen/test data\n",
        "\n",
        "\n",
        "Scaling and Transforming New Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "alMf73WCPDTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Orx2hGYO04I"
      },
      "outputs": [],
      "source": []
    }
  ]
}